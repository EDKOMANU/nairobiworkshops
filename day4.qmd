---
title: "SAE using geospatial data"
subtitle: "Nairobi Workshop: Day 4 (geospatial data)"
author: 
  - name: Ann-Kristin Kreutzmann
  - name: Josh Merfeld
date: "26 August 2024"
date-format: long
format: 
  revealjs:
    self-contained: true
    slide-number: false
    progress: false
    theme: [serif, custom.scss]
    width: 1500
    code-copy: true
    code-fold: show
    code-overflow: wrap
    highlight-style: github
execute:
  echo: false
  warnings: false

# to print as pdf: localhost:6106/?print-pdf (will have to change the local host number)
---

```{r}
#| label: setup
#| include: false
widthdefault <- getOption("width")
options(width = 1500)
library(tidyverse)
library(sf)
library(cowplot)
library(haven)
library(terra)
library(tidyterra)
library(exactextractr)

Sys.setenv("RETICULATE_PYTHON" = "~/Library/r-miniconda-arm64/envs/RGEEDIM/bin/python3.9")
library(rgeedim)
# short duration token
gd_authenticate(auth_mode = "gcloud")
# initialize and should be good to go 
gd_initialize()

```

## Introduction to geospatial data

-   [One estimate](https://www.ibm.com/topics/geospatial-data) says that 100 TB of only weather data are generated every single day
    -   This means there is a lot of data to work with!
    -   Note that this is also problematic, since it can be difficult to work with such large datasets
-   Geospatial data is used in a variety of fields
    -   Agriculture
    -   Urban planning
    -   Environmental science
    -   Public health
    -   Transportation
    -   And many more!

## The amount of geospatial data is useful for SAE

-   Geospatial data can be highly predictive of e.g. poverty
    -   Urbanity
    -   Land class/cover
    -   Vegetation indices
    -   Population counts
    -   etc. etc.
-   More importantly: it's available everywhere!

## Think of what you need for SAE

-   You need a sample, e.g. a household survey
    -   This will only cover some of the country<br><br>
-   You need auxiliary data that is:
    -   Predictive of the outcome you care about
    -   Available throughout the entire country<br><br>
-   Some countries, use administrative data
    -   But, importantly, it's often not available or is of low quality!

## A quick example

-   Let's take a look at Malawi

-   Why Malawi?

    -   I have survey data you can use ðŸ˜ƒ
    - Only going to use part of Malawi for this example (size of data)

-   Consider the 2019/2020 Integrated Household Survey (IHS5)

    -   Was used for the Malawi Poverty Report 2020
    -   Can say things about poverty at the district level
    - If you want to split by urban/rural, only at the region level

## A quick example

![](day4assets/malawipov.png){fig-align="center"}


## Malawi admin areas - Northern region only

::: columns
::: {.column width="70%"}
```{r}
#| echo: false
#| include: true
#| fig-align: center

admin2 <- read_sf("day4data/mw2.shp")
admin3 <- read_sf("day4data/mw3.shp")
admin4 <- read_sf("day4data/mw4.shp")

g1 <- ggplot() + 
  geom_sf(data = admin2, fill = "white", color = "black") +
  theme_bw() +
  labs(subtitle = "District (admin2)")
g2 <- ggplot() + 
  geom_sf(data = admin3, fill = "white", color = "black") +
  theme_bw() +
  labs(subtitle = "TA (admin3)")
g3 <- ggplot() + 
  geom_sf(data = admin4, fill = "white", color = "black") +
  theme_bw() +
  labs(subtitle = "EA (admin4)")

plot_grid(g1, g2, g3, ncol = 3)
```
:::

::: {.column width="30%"}
-   Survey only lets us say things about the districts!
-   What if we want to say something about traditional authorities (TAs)?
-   Individual TAs might not have enough observations
-   We could use SAE! But what auxiliary data?
:::
:::


## Observations at the district and TA level

```{r}
#| echo: false
#| include: true
#| fig-align: center

admin2 <- read_sf("day4data/mw2.shp")
admin3 <- read_sf("day4data/mw3.shp")
admin3 <- st_transform(admin3, crs = st_crs(admin2))
survey <- read_dta("day4data/householdgeovariables_ihs5.dta")
survey$DIST_CODE <- substr(survey$case_id, 1, 3)
survey$TA_CODE <- substr(survey$case_id, 1, 5)

temp <- survey |>
  group_by(DIST_CODE) |>
  summarise(n = n())
admin2 <- admin2 |>
  left_join(temp, by = "DIST_CODE")

temp <- survey |>
  group_by(TA_CODE) |>
  summarise(n = n())
admin3 <- admin3 |>
  left_join(temp, by = "TA_CODE")

admin3$n[is.na(admin3$n)] <- 0

g1 <- ggplot() + 
  geom_sf(data = admin2, aes(fill = n), color = NA, lwd = 0.001) +
  scale_fill_distiller("Observations", palette = "Spectral") +
  theme_bw() +
  labs(subtitle = "District (admin2)")
g2 <- ggplot() + 
  geom_sf(data = admin3, aes(fill = n), color = NA, lwd = 0.001) +
  scale_fill_distiller("Observations", palette = "Spectral") +
  theme_bw() +
  labs(subtitle = "TA (admin3)")

# extract the legend from one of the plots
legend <- ggpubr::get_legend(g1 +
  # horizontal legend
  guides(color = guide_legend(nrow = 1)) +
  theme(legend.position = "bottom"))
grid <- plot_grid(g1 + theme(legend.position = "none"), g2 + theme(legend.position = "none"), ncol = 2)
plot_grid(grid, legend, ncol = 1, rel_heights = c(1, 0.1))
```







## Sub-area model with sectors

-   One option: estimate a sub-area model at the EA level!

-   Steps:

    -   Collapse survey data to the EA level
    -   Extract geospatial data at the EA level
    -   Estimate the model
    
    
  

# Getting started with <br>geospatial data {#color-slide background-image="day4assets/titlebackground.jpg"}

```{css, echo=FALSE}
#color-slide, 
#color-slide h1 {
 color: white;
 text-align: left;
}
```

  
  
  
## Getting started with geospatial data

- Due to time, this introduction will be necessarily brief

- We are going to learn about the following:
  - Shapefiles
  - Rasters
  - Extracting data


  
  
  
## Shapefiles

- The maps I just showed you are `shapefiles`

- Shapefiles are a common format for geospatial data
  - They are a form of `vector` data

- Shapefiles are made up of _at least_ four files:
  - `.shp` - the shape itself
  - `.shx` - the index
  - `.dbf` - the attributes
  - `.prj` - the projection
  - What these all mean isn't important for now, just make sure they are there! Check the `day4data` folder on github.


  
  
  
## Let's look at Northern Malawi again

```{r}
#| echo: false
#| include: true
#| fig-align: center

admin2 <- read_sf("day4data/mw2.shp")

g1 <- ggplot() + 
  geom_sf(data = admin2, fill = "white", color = "gray") +
  geom_sf(data = admin2 |> filter(DIST_CODE=="107"), fill = "white", color = "black") +
  theme_bw() +
  labs(subtitle = "Districts in Northern\nMalawi")
g2 <- ggplot() + 
  geom_sf(data = admin2 |> filter(DIST_CODE=="107"), fill = "white", color = "black") +
  theme_bw() +
  labs(subtitle = "Mzuzu City")

plot_grid(g1, g2, ncol = 2, rel_widths = c(0.7, 1))
```

- Left: collection of `features`
- Right: one `feature`


  
  
  
## Features are made of vertices, which connect

```{r}
#| echo: false
#| include: true
#| fig-align: center

admin2 <- read_sf("day4data/mw2.shp")
vertices <- spatialEco::extract.vertices(admin2 |> filter(DIST_CODE=="107"))

g1 <- ggplot() + 
  geom_sf(data = admin2 |> filter(DIST_CODE=="107"), fill = "white", color = "black") +
  theme_bw() +
  labs(subtitle = "Mzuzu City")
g2 <- ggplot() + 
  geom_sf(data = admin2 |> filter(DIST_CODE=="107"), fill = "white", color = "black") +
  geom_sf(data = vertices, color = "black", size = 0.5) +
  theme_bw() +
  labs(subtitle = "Mzuzu City\n489 vertices!")

plot_grid(g1, g2, ncol = 2, rel_widths = c(0.7, 1))
```


  
  
  
## Imagine a rectangle, on a coordinate plane

```{r}
#| echo: false
#| include: true
#| fig-align: center

square <- data.frame(
  x = c(1, 1, 3, 3),
  y = c(1, 3, 3, 1)
)

ggplot() +
  geom_point(data = square, aes(x = x, y = y), color = NA) +
  scale_x_continuous(breaks = c(0, 1, 2, 3, 4), minor_breaks = NULL, limits = c(0, 4)) +
  scale_y_continuous(breaks = c(0, 1, 2, 3, 4), minor_breaks = NULL, limits = c(0, 4)) +
  theme_bw() +
  # add text
  annotate("text", x = 2, y = 2, label = "How many points do we need\nto create a rectangle on this\ncoordinate plane?")


```


  
  
  
## Imagine a rectangle, on a coordinate plane

```{r}
#| echo: false
#| include: true
#| fig-align: center

square <- data.frame(
  x = c(1, 1, 3, 3),
  y = c(1, 3, 3, 1)
)

ggplot() +
  geom_point(data = square, aes(x = x, y = y), color = "black") +
  scale_x_continuous(breaks = c(0, 1, 2, 3, 4), minor_breaks = NULL, limits = c(0, 4)) +
  scale_y_continuous(breaks = c(0, 1, 2, 3, 4), minor_breaks = NULL, limits = c(0, 4)) +
  theme_bw() +
  # add text
  annotate("text", x = 2, y = 2, label = "Four points!")


```


  
  
  
## Imagine a rectangle, on a coordinate plane

::: columns
::: {.column width="50%"}

- We need four points.
- But features in shapefiles are a little different.
  - We have to "close" the feature
- We do this by adding a fifth point: the same as the first point!


```{=html}
<table style="width:100%">
  <caption><span data-qmd="Five points (vertices) in our feature"></span></caption>
  <thead>
    <tr style="border-top:1px solid black">
      <th><span data-qmd=" "></span></th>
      <th><span data-qmd="X value"></span></th>
      <th><span data-qmd="Y value"></span></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Point 1</td>
      <td style='text-align:center; vertical-align:middle'> 1 </td>
      <td style='text-align:center; vertical-align:middle'> 1 </td>
    </tr>
    <tr>
      <td>Point 2</td>
      <td style='text-align:center; vertical-align:middle'> 3 </td>
      <td style='text-align:center; vertical-align:middle'> 1 </td>
    </tr>
    <tr>
      <td>Point 3</td>
      <td style='text-align:center; vertical-align:middle'> 3 </td>
      <td style='text-align:center; vertical-align:middle'> 3 </td>
    </tr>
    <tr>
      <td>Point 4</td>
      <td style='text-align:center; vertical-align:middle'> 1 </td>
      <td style='text-align:center; vertical-align:middle'> 3 </td>
    </tr>
    <tr>
      <td>Point 5</td>
      <td style='text-align:center; vertical-align:middle'> 1 </td>
      <td style='text-align:center; vertical-align:middle'> 1 </td>
    </tr>
  </tbody>
</table>
```


:::

::: {.column width="50%"}

```{r}
#| echo: false
#| include: true
#| fig-align: center

square <- data.frame(
  x = c(1, 1, 3, 3),
  y = c(1, 3, 3, 1)
)

ggplot() +
  geom_point(data = square, aes(x = x, y = y), color = "black") +
  scale_x_continuous(breaks = c(0, 1, 2, 3, 4), minor_breaks = NULL, limits = c(0, 4)) +
  scale_y_continuous(breaks = c(0, 1, 2, 3, 4), minor_breaks = NULL, limits = c(0, 4)) +
  theme_bw()


```

:::
:::





## Features are made of vertices, which connect

::: columns
::: {.column width="50%"}

- So we have all our vertices (489 of them!)
- The question:
  - What is the coordinate system here?

:::

::: {.column width="50%"}

```{r}
#| echo: false
#| include: true
#| fig-align: center

admin2 <- read_sf("day4data/mw2.shp")
vertices <- spatialEco::extract.vertices(admin2 |> filter(DIST_CODE=="107"))

ggplot() + 
  geom_sf(data = admin2 |> filter(DIST_CODE=="107"), fill = "white", color = "black") +
  geom_sf(data = vertices, color = "black", size = 0.5) +
  theme_bw() +
  labs(subtitle = "Mzuzu City\n489 vertices!")

```

:::
:::





## Latitude and longitude on a globe

- The most common `coordinate reference system (CRS)` is latitude/longitude
  - Latitude: North/South
  - Longitude: East/West
  - The equator is at 0Â° latitude
  - The prime meridian is at 0Â° longitude<br><br>
  
- But there's a problem with using latitude/longitude
  - The Earth is a sphere (well, more or less; really an oblate spheroid)





## {background-image="day4assets/worldmap.png"}


## {background-image="day4assets/worldmap2.png"}





## The basic problem

- The basic problem is that one degree of longitude changes at different latitudes!
  - At the equator, one degree of longitude is about 111 km
  - At 15N/S, one degree of longitude is about 107 km
  - At 30N/S, one degree of longitude is about 96 km
  - At 45N/S, one degree of longitude is about 79 km
  - At 60N/S, one degree of longitude is about 56 km
    - This explains Greenland!<br><br>
    
- It's not an easy problem to solve, as all solutions have drawbacks!





## Preserve shape, give up area

```{r}
#| echo: false
#| include: true
#| fig-align: center

temp <- magick::image_read("day4assets/crs1.png")
temp2 <- magick::image_read("day4assets/crs2.png")


g1 <- magick::image_ggplot(temp) +
  labs(subtitle = "A. Mercator projection") +
  theme_void()
g2 <- magick::image_ggplot(temp2) +
  labs(subtitle = "B. Lambert projection") +
  theme_void()

plot_grid(g1, NA, g2, ncol = 3, rel_widths = c(1, 0.05, 1))

```





## Long story short...

- Using lat/lon is generally fine as long as you don't care about distances
  - But if you do, you need to use a different CRS<br><br>

- Today we will focus on things that do not require distances
  - So we will generally use lat/lon







## Reading shapefiles in R

- My go-to package for shapefiles in R is `sf`
- Reading shapefiles is VERY easy! And you can treat them like dataframes.

```{r}
#| echo: true
#| include: true
#| fig-align: center
library(sf)
# this is the shapefile for the northern region of Malawi, district level
northmw <- read_sf("day4data/mw2.shp")
northmw
```







## Plotting is also very easy


```{r}
#| echo: true
#| include: true
#| fig-align: center
ggplot() + 
  geom_sf(data = northmw)
```








## My go-to theme


```{r}
#| echo: true
#| include: true
#| fig-align: center
ggplot() + 
  geom_sf(data = northmw, fill = NA, color = "black") +
  theme_bw() +
  labs(subtitle = "Districts in Northern Malawi")
```








## Give it a try with TAs (mw3.shp)

```{r}
#| eval: false
#| echo: true
#| code-fold: show
library(sf)
# this is the shapefile for the northern region of Malawi, TA level
northmw <- read_sf("day4data/mw3.shp")
ggplot() +
  geom_sf(data = northmw)
```

```{r}
#| eval: false
#| echo: true
#| code-fold: show

ggplot() +
  geom_sf(data = northmw, fill = NA, color = "black") +
  theme_bw() +
  labs(subtitle = "TAs in Northern Malawi")
```






## One more example - map from earlier


::: columns
::: {.column width="45%"}

```{r}
#| echo: true
#| eval: false
#| fig-align: center

admin2 <- read_sf("day4data/mw2.shp")

ggplot() + 
  geom_sf(data = admin2, 
    fill = "white", color = "gray") +
  geom_sf(data = admin2 |> filter(DIST_CODE=="107"), 
    fill = "white", color = "black") +
  theme_bw() +
  labs(subtitle = "Districts in Northern Malawi")
```

:::

::: {.column width="55%"}

![](day4assets/exampleplot.png){fig-align="center"}

:::
:::






## Rasters

- We've discussed shapefiles
  -  Now let's talk about rasters!<br><br>
  
- Rasters are a different type of geospatial data
  - They are made up of a grid of cells
  - Each cell has a value






## Example raster grid - how much info do we need?

```{r}
#| echo: false
#| include: true
#| fig-align: center
# create example grid
main <- ggplot() +
  geom_hline(yintercept = 1:10, color = "black") +
  geom_vline(xintercept = 1:10, color = "black") +
  theme_minimal() +
  labs(x = "X", y = "Y") +
  scale_x_continuous(breaks = 1:10, minor_breaks = NULL) +
  scale_y_continuous(breaks = 1:10, minor_breaks = NULL)

gridexample <- ggdraw() +
  draw_plot(
    {
      main +
        coord_sf(
          xlim = c(0.99, 10.01),
          ylim = c(0.99, 10.01),
          expand = FALSE)
    }
)
gridexample
```


- Here's a grid.
  - How many points do we need?






## Example raster grid - how much info do we need?

```{r}
#| echo: false
#| include: true
#| fig-align: center


gridexample
```


- Need to know location of one grid cell...
  - And the size of each grid!






## How much info do we need?

- In other words, we do not need a point for every raster cell<br><br>

- We just need to know:
  - The location of one cell
  - The size of each cell
    - This is called the `resolution` of the raster<br><br>
    
- Example:
  - I know the first grid cell in bottom left is at (0, 0)
  - I know each grid cell is 1 meter by 1 meter (the resolution)
  - Then I know the exact location of every single grid cell






## Population in Cotonou, Benin

```{r}
#| echo: false
#| include: true
#| fig-align: center

library(tidyterra)

tif <- rast("day4data/beninpop.tif")

ggplot() +
  geom_spatraster(data = tif, ) + 
  scale_fill_distiller("Population\ncount", palette = "Spectral", na.value = "white") +
  theme_minimal() +
  labs(subtitle = "Population in Cotonou, Benin")

```

- What are the white values?






## Population in Cotonou, Benin

- Here's the information for this raster
  - What's the resolution? What are the units?


```{r}
#| echo: false
#| include: true
#| fig-align: center

tif

```






## Rasters

- Rasters are defined by the grid layout and the resolution
  - Grid cells are sometimes called pixels (just like images, which are often rasters!)<br><br>

- There are many different file types for rasters
  - `.tif` or `.tiff` (one of the most common)
  - `.nc` (NetCDF, common for very large raster data)
  - Image files, e.g. `png`, `jpg`, etc.<br><br>






## Reading rasters in R

- Reading rasters is also quite easy!
  - Going to use the `terra` package for it
    - Note: can use `terra` for shapefiles, too
  - `day4data/beninpop.tif` is a raster of population counts in Benin

```{r}
#| echo: true
#| include: true
#| fig-align: center
library(terra)

# this is the raster for Cotonou, Benin
cotonou <- rast("day4data/beninpop.tif")
cotonou
```







## Plotting rasters

::: columns
::: {.column width="45%"}

- Plotting rasters only with `terra` is a bit of a pain
  - Can't use `ggplot`
  - So, I load another package that lets me use `ggplot` with rasters
    - `tidyterra`

```{r}
#| echo: true
#| eval: false
#| fig-align: center
library(tidyterra)

ggplot() +
  geom_spatraster(data = cotonou)
```

:::

::: {.column width="55%"}

```{r}
#| echo: false
#| include: true
#| fig-align: center
library(tidyterra)

ggplot() +
  geom_spatraster(data = cotonou)
```

:::
:::







## Making it nicer

::: columns
::: {.column width="45%"}

```{r}
#| echo: true
#| eval: false
#| fig-align: center
library(tidyterra)

ggplot() +
  geom_spatraster(data = cotonou) + 
  # distiller is for continuous values
  # but we can use palettes!
  # I like spectral a lot
  scale_fill_distiller("Population\ncount", 
    palette = "Spectral", na.value = "white") +
  theme_minimal() +
  labs(subtitle = "Population in Cotonou, Benin")
```

:::

::: {.column width="55%"}

```{r}
#| echo: false
#| include: true
#| fig-align: center
library(tidyterra)

ggplot() +
  geom_spatraster(data = cotonou) + 
  scale_fill_distiller("Population\ncount", palette = "Spectral", na.value = "white") +
  theme_minimal() +
  labs(subtitle = "Population in Cotonou, Benin")

```

:::
:::







## Extracting raster data to shapefiles

- Let's go back to our use case:
  - We want to estimate a sub-area model at the EA level in Malawi
  - This means we need to extract raster data to the EA level
  - We can do this with `terra`, `sf`, and `exactextractr`
    - `terra` has its own method, but i find `exactextractr` to be MUCH faster<br><br>
    
- Let's start by looking at the raster I've uploaded to the `day4data`: `mwpop.tif`







## Give it a try

::: columns
::: {.column width="45%"}

- Try to load it into R using terra, then plot it with tidyterra and ggplot

```{r}
#| echo: true
#| eval: false
#| fig-align: center
#| code-fold: show

tif <- rast("day4data/mwpop.tif")

ggplot() +
  geom_spatraster(data = tif) + 
  scale_fill_distiller("Population\ncount", 
    palette = "Spectral", na.value = "white") +
  theme_minimal() +
  labs(subtitle = "Population in Northern Malawi")
```

:::

::: {.column width="55%"}

![](day4assets/mwpop.png){fig-align="center"}

:::
:::







## Give it a try

::: columns
::: {.column width="45%"}

- I actually don't like that map! It's too hard to see because of all the low values.
- So let's take logs, instead!
  - Note that all the zeros become missing (can't log zero)

```{r}
#| echo: true
#| eval: false
#| fig-align: center
#| code-fold: show

tif <- rast("day4data/mwpop.tif")

ggplot() +
  geom_spatraster(data = log(tif)) + 
  scale_fill_distiller("Population\ncount (log)", 
    palette = "Spectral", na.value = "white") +
  theme_minimal() +
  labs(subtitle = "Population in Northern Malawi")
```

:::

::: {.column width="55%"}

![](day4assets/mwpop2.png){fig-align="center"}

:::
:::







## We want to extract the .tif values to the .shp

```{r}
#| echo: false
#| eval: true
#| fig-align: center

tif <- rast("day4data/mwpop.tif")
adm4 <- read_sf("day4data/mw4.shp")

g1 <- ggplot() +
  geom_spatraster(data = tif) + 
  scale_fill_distiller("Population\ncount (log)", 
    palette = "Spectral", na.value = "white") +
  theme_minimal() +
  labs(subtitle = "Population in Northern Malawi")
g2 <- ggplot() +
  geom_sf(data = adm4, color = "black", fill = "transparent", lwd = 0.1) + 
  theme_minimal() +
  labs(subtitle = "EAs (admin4) Northern Malawi")

plot_grid(g1, NA, g2, ncol = 3, rel_widths = c(1, 0.05, 1))
```







## Let's do it with `exactextractr`

```{r}
#| echo: true
#| eval: false
#| include: true
#| fig-align: center
library(exactextractr)

tif <- rast("day4data/mwpop.tif")
adm4 <- read_sf("day4data/mw4.shp")
# make sure they are in the same CRS! (they already are, but just in case)
# st_transform is for the sf object
adm4 <- st_transform(adm4, crs = crs(tif))

# extract the raster values to the shapefile
# we are going to SUM, and add the EA_CODE from the shapefile to the result
extracted <- exact_extract(tif, adm4, fun = "sum", append_cols = "EA_CODE")
```

```{r}
#| echo: false
#| eval: true
#| include: false
#| fig-align: center
library(exactextractr)

tif <- rast("day4data/mwpop.tif")
adm4 <- read_sf("day4data/mw4.shp")
# make sure they are in the same CRS! (they already are, but just in case)
# st_transform is for the sf object
adm4 <- st_transform(adm4, crs = crs(tif))

# extract the raster values to the shapefile
# we are going to SUM, and add the EA_CODE from the shapefile to the result
extracted <- exact_extract(tif, adm4, fun = "sum", append_cols = "EA_CODE")
# save it!
write_csv(extracted |> rename(pop = sum), "day4data/mwpopEAs.csv")
```

```{r}
#| echo: true
#| eval: true
#| include: true
#| fig-align: center

head(extracted)
```







## Now we can join the extracted data to the shapefile


::: columns
::: {.column width="45%"}


```{r}
#| echo: true
#| eval: false
#| include: true
#| fig-align: center

# join
adm4 <- adm4 |>
  left_join(extracted, by = "EA_CODE")

# plot it!
ggplot() +
  geom_sf(data = adm4, aes(fill = sum), 
    color = "black", lwd = 0.01) +
  scale_fill_distiller("Population\ncount", 
    palette = "Spectral", na.value = "white") +
  theme_minimal() +
  labs(subtitle = "Population in EAs")

```

:::

::: {.column width="55%"}

![](day4assets/mwpopEAs.png){fig-align="center"}

:::
:::







## Now it's your turn

- Here's your task:
  - Search for "worldpop population counts"
    - Should be the first result (link: [https://hub.worldpop.org/project/categories?id=3](https://hub.worldpop.org/project/categories?id=3))
  - Scroll down the page, click on "unconstrained individual countries 2000-2020 UN adjusted (1km resolution)

![](day4assets/worldpoppage1.png){fig-align="center"}






## Now it's your turn

- Here's your task:
  - Search for "worldpop population counts"
    - Should be the first result (link: [https://hub.worldpop.org/project/categories?id=3](https://hub.worldpop.org/project/categories?id=3))
  - Scroll down the page, click on "unconstrained individual countries 2000-2020 UN adjusted (1km resolution)
  - Then, search for a country (maybe yours?)

![](day4assets/worldpoppage2.png){fig-align="center"}





## Now it's your turn

- Here's your task:
  - Search for "worldpop population counts"
    - Should be the first result (link: [https://hub.worldpop.org/project/categories?id=3](https://hub.worldpop.org/project/categories?id=3))
  - Scroll down the page, click on "unconstrained individual countries 2000-2020 UN adjusted (1km resolution)
  - Then, search for a country (maybe yours?)
  - Click on "Data & Resources" for 2020
  - Scroll down to the bottom of the page and download the .tif





## Now it's your turn

- Load the .tif into R using `terra`
- Plot the raster using `tidyterra` and `ggplot`
  - Make it look nice!





## Let's keep going!

- Now you need to find a shapefile for the same country
- This will be a bit less straightforward
  - Search for "shapefile COUNTRY humdata"
  - You should find a link to the Humanitarian Data Exchange
  - Click on it and see if it has shapefiles for your country of choice
  - If so, download a shapefile (it can be at a higher admin level)
    - If not, raise your hand and I'll come help you find a shapefile
  - Load it into R and plot it!





## One last thing

- You have the population tif and the shapefile
- Extract the population data (using sum, don't forget!) to the shapefile
  - Use `append_cols` and make sure you choose the correct identifier!
- Join the data to the shapefile
- Plot the shapefile with the population data
  - Make it look nice!





## What can you do with that data?

- Now you have a shapefile with population data
- You can save it as a `.csv` and use it in your analysis!
  - We'll get to this point eventually.
  - We will also discuss adding the survey data and then estimating a sub-area model





# Finding rasters {background-image="day4assets/ndvibackground.png" background-position="right"}


## Where can you find rasters?

- Depending on the variable, rasters are sometimes quite easy to find!
  - We already saw one example: WorldPop (population counts)<br><br>

- There are two large online repositories:
  - [Google Earth Engine](https://developers.google.com/earth-engine/datasets)
  - [Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/)
    - This one is newer and has less data (for now)



## {background-image="day4assets/gee1.png"}

## Google Earth Engine

- Google Earth Engine has _a lot_ of data.

- Let's see some examples
    
    
  

## Surface temperature {#color-slide2 background-image="day4assets/geeexample1.gif"}

```{css, echo=FALSE}
#color-slide2, 
#color-slide2 h2 {
 color: white;
 text-align: left;
}
```


## Climate {#color-slide3 background-image="day4assets/geeexample2.gif"}

```{css, echo=FALSE}
#color-slide3, 
#color-slide3 h2 {
 color: white;
 text-align: left;
}
```

## Land cover {#color-slide4 background-image="day4assets/geeexample3.gif"}

```{css, echo=FALSE}
#color-slide4, 
#color-slide4 h2 {
 color: white;
 text-align: left;
}
```

## Imagery {#color-slide5 background-image="day4assets/geeexample4.jpg"}

```{css, echo=FALSE}
#color-slide5, 
#color-slide5 h2 {
 text-align: left;
}
```



## GeoLink

- I'm going to start with some example code for a package called `GeoLink`
  - This package actually uses Microsoft Planetary Computer
  - It is very very easy to use
  - The problem: as of now, it only has a few datasets

```{r}
#| eval: false
#| echo: true

# we have to download it differently
devtools::install_github("SSA-Statistical-Team-Projects/GeoLink")

```



## The actual code is much easier to use than GEE!

```{r}
#| eval: false
#| echo: true

library(GeoLink)
library(sf)

adm4 <- read_sf("mw4.shp")
# CHIRPS is rainfall data
adm4_chirps <- geolink_chirps(time_unit = "month",
  start_date = "2019-01-01",
  end_date = "2019-03-01",
  shp_dt = adm4,
  extract_fun = "mean")
summary(adm4_chirps)

```

- Keep an eye on this package! It will be very useful when it has more datasets




## Google Earth Engine

- First things first: you need an account!<br><br>

- Go to [https://earthengine.google.com/](https://earthengine.google.com/) and sign up
  - Top-right corner: `Get Started`
  - Next page: `Create account`<br><br>
  
- I'll give you all a couple minutes to get this set up. Let me know if you have problems



## Let's look at a dataset

- On the [https://earthengine.google.com/](https://earthengine.google.com/) page:
  - Click `View all datasets` (near the top)
  - Search for `lights`
  - We want `VIIRS Nighttime Day/Night Annual Band Composites V2.1`

![](day4assets/gee2.png){fig-align="center"}



## Basic information about the dataset

![](day4assets/gee3.png){fig-align="center"}



## Raster bands - They can have more than one!

![](day4assets/gee4.png){fig-align="center"}





## We need to download python... sorry!

- The first time you use `rgeedim`, you will need to install python
  - The easiest way is to search `download miniconda`
  - One of the first links should be [https://docs.anaconda.com/miniconda/](https://docs.anaconda.com/miniconda/)
  - Go down to "Latest Miniconda installer links" and select the correct link for your platform (e.g. Windows)
  - Once it finishes downloading, please go do your downloads folder and double click to install<br><br>
  
- Let's take five minutes to download and install python



## Downloading the data is... a pain

- Actually getting the data is a bit of a pain
  - Unless you know Javascript!<br><br>
  
- A lot of people use libraries in `R` (or python) to download data, instead.
  - All of them are a bit cumbersome.
    - Especially true in `R`, because we need to use python!
  - We are going to use `rgeedim`
  - Go ahead and install it using `install.packages("rgeedim")`
  - Load it using `library(rgeedim)`
    - Type `Yes` when asked to create a default Python environment




## The code

```{r}
#| eval: false
#| echo: true

library(rgeedim)
gd_install()
gd_authenticate(auth_mode = "notebook")
```

- After `gd_authenticate()`, your browser should open.
  - You'll need to sign in to your Google account
  - Continue through the prompts and make sure you select all access




## The code

```{r}
#| eval: false
#| echo: true

library(rgeedim)
gd_install()
gd_authenticate(auth_mode = "notebook")
```

- After `gd_authenticate()`, your browser should open.
  - You'll need to sign in to your Google account
  - Continue through the prompts and make sure you select all access



## The code

::: columns
::: {.column width="45%"}

- You'll arrive at this page.
- Click the `Copy` button

```{r}
#| eval: false
#| echo: true

library(rgeedim)
gd_install()
gd_authenticate(auth_mode = "notebook")
```

- Then go back to RStudio, and paste (ctrl + v) the code into the console

:::

::: {.column width="55%"}

![](day4assets/geelogin.png){fig-align="center"}

:::
:::




## The code

```{r}
#| eval: false
#| echo: true

library(rgeedim)
#gd_install() # You SHOULD NOT need to do this on each new session
gd_authenticate(auth_mode = "notebook") # need to do this
gd_initialize() # and you need to do this
```

- After you do `gd_install()` once, you should be good
  - You will need to do `gd_authenticate()` and `gd_initialize()` each time you start a new session




## Downloading the data - still not straightforward!

- First, we need to create a "bounding box"
  - This is the area of the globe we want to search
  - We will use the Malawi shapefile for this
  - The "bounding box" is a rectangle that completely contains the shapefile
  
```{r}
#| eval: true
#| echo: true

# load shapefile
malawi <- read_sf("day4data/mw4.shp")
# this creates the bounding box
bbox <- st_bbox(malawi)
bbox

```







## Basic information about the dataset

- Remember I said we'd need this again?

![](day4assets/gee3b.png){fig-align="center"}








## Image collections vs. images

- One key thing to understand about GEE is the difference between image collections and images<br><br>

- An image collection is what it sounds like: a collection of images
  - The key is that we won't download image collections
  - We'll download individual images
  - So we need to find the images!
  







## Get images from the collection

```{r}
#| eval: true
#| echo: true

x <- gd_collection_from_name("NOAA/VIIRS/DNB/ANNUAL_V21") |>
  gd_search(region = bbox)
gd_properties(x)

```

- The survey data I have from Malawi is 2019/2020, so let's download the 2019-01-01 data
  - We want to use the id: `NOAA/VIIRS/DNB/ANNUAL_V21/20190101`
  







## We can FINALLY download the raster!



```{r}
#| eval: true
#| echo: true

x <- gd_image_from_id("NOAA/VIIRS/DNB/ANNUAL_V21/20190101") |>
  gd_download(
    filename = "temp.tif",
    region = bbox, # region is our bbox
    scale = 500, # resolution of raster is only 500, so no reason to go lower
    crs = 'EPSG:4326', # lat/lon
    overwrite = TRUE, # overwrite if it exists
    silent = FALSE
  )
# we downloaded the raster and called it x
# so let's load it using terra!
x <- rast(x)
# here it is!
x
```




  







## Quick note: we downloaded many bands!



```{r}
#| eval: true
#| echo: true
#| class-output: hscroll

names(x)

# but we really only want average nightlights
# so here's how you can download just the average
x <- gd_image_from_id("NOAA/VIIRS/DNB/ANNUAL_V21/20190101") |>
  gd_download(
    filename = "temp.tif",
    region = bbox, # region is our bbox
    scale = 500, # resolution of raster is only 500, so no reason to go lower
    crs = 'EPSG:4326', # lat/lon
    overwrite = TRUE, # overwrite if it exists
    silent = FALSE,
    bands = list("average"),
  )
# we downloaded the raster and called it x
# so let's load it using terra!
x <- rast(x)
# here it is!
x
```



  







## What does it look like?


::: columns
::: {.column width="45%"}

```{r}
#| eval: false
#| echo: true
adm4 <- read_sf("day4data/mw4.shp")
ggplot() +
  geom_spatraster(data = x) +
  scale_fill_distiller("Nightlights", 
    palette = "Spectral") +
  geom_sf(data = adm4, 
    color = "white",
    lwd = 0.01,
    alpha = 0.5,
    fill = "transparent") +
  theme_minimal() +
  labs(subtitle = "Nightlights in Malawi")
```

- Note what the "bounding box" does!
  - `st_bbox` as a reminder

:::

::: {.column width="55%"}

![](day4assets/rgeedim1.png){fig-align="center"}

:::
:::


  
  
  
  



  







## We want to extract NTL to the shapefile


```{r}
#| eval: false
#| echo: true
adm4 <- read_sf("day4data/mw4.shp")
# exact_extract
library(exactextractr)
ntlextract <- exact_extract(x, adm4, fun = "mean", append_cols = "EA_CODE")
head(ntlextract)
# save it!
write_csv(ntlextract |> rename(ntl = mean), "day4data/mwntlEAs.csv")

```








## Now it's your turn!

- We want to download NDVI data for Malawi
  - We want the `Terra Vegetation Indices 16-Day Global 500m` data (you can just search this)
  - Download the first observation from 2019
  - Extract it to the mw4 shapefile!




```{r}
#| eval: false
#| echo: true
#| code-fold: show

# load shapefile
malawi <- read_sf("day4data/mw4.shp")
# this creates the bounding box
bbox <- st_bbox(malawi)

x <- gd_collection_from_name("MODIS/061/MOD13A1") |>
  gd_search(region = bbox)
gd_properties(x)
# the ID for the first image of 2019 is MODIS/061/MOD13A1/2019_01_01

```



## Let's download that id


::: columns
::: {.column width="55%"}

```{r}
#| eval: false
#| echo: true

x <- gd_image_from_id("MODIS/061/MOD13A1/2019_01_01") |>
  gd_download(
    filename = "temp.tif",
    region = bbox, # region is our bbox
    scale = 500, # resolution
    crs = 'EPSG:4326', # lat/lon
    overwrite = TRUE, # overwrite if it exists
    bands = list("NDVI") # only download NDVI
  )
x <- rast(x) # load raster
ndviextract <- exact_extract(x, malawi, fun = "mean", append_cols = "EA_CODE")
malawi <- malawi |>
  left_join(ndviextract |> rename(ndvi = mean), by = "EA_CODE")
ggplot() +
  geom_sf(data = malawi, aes(fill = ndvi), lwd = 0.01,) +
  scale_fill_distiller("NDVI", 
    palette = "Greens", direction = 1) +
  theme_minimal()
```

```{r}
#| eval: true
#| echo: false
#| include: false
#| class-output: hscroll

x <- gd_image_from_id("MODIS/061/MOD13A1/2019_01_01") |>
  gd_download(
    filename = "temp.tif",
    region = bbox, # region is our bbox
    scale = 500, # resolution
    crs = 'EPSG:4326', # lat/lon
    overwrite = TRUE, # overwrite if it exists
    bands = list("NDVI") # only download NDVI
  )
x <- rast(x) # load raster
ndviextract <- exact_extract(x, malawi, fun = "mean", append_cols = "EA_CODE")
malawi <- malawi |>
  left_join(ndviextract |> rename(ndvi = mean), by = "EA_CODE")
ggplot() +
  geom_sf(data = malawi,
    aes(fill = ndvi),
    lwd = 0.01,) +
  scale_fill_distiller("NDVI", 
    palette = "Greens", direction = 1) +
  theme_minimal()
```


:::

::: {.column width="45%"}

![](day4assets/rgeedimndvi.png){fig-align="center" width="65%"}

:::
:::



  







## This returns a LOT of results - search by date!


```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

# load shapefile
malawi <- read_sf("day4data/mw4.shp")
# this creates the bounding box
bbox <- st_bbox(malawi)

# so let's filter by date!
x <- gd_collection_from_name("MODIS/061/MOD13A1") |>
  gd_search(region = bbox,
    start_date = "2019-01-01",
    end_date = "2019-12-31")
gd_properties(x)

```

  







## Let's look at the dates


```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

gd_properties(x)$date

```

- What should we download?
  - NDVI is a vegetation index, which means it varies quite a bit throughout the year
  - We could take average NDVI throughout the year
  - Or we could take NDVI at a specific time of year
  - Or we could take the max... or the min... or all of the above!
  
- Let's download one raster PER MONTH
  - How?
  
  
  
  
  
  
## Let's look at the dates

- There are `r length(gd_properties(x)$date)` dates
- This code is a bit complicated, so let me explain


```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll
# get the dates
dates <- gd_properties(x)$date
# here are the months
months <- month(dates)
ids <- c() # this creates an empty vector
for (m in 1:12){ # this "for loop" loops through each month (1 through 12)
  ids <- c(ids, which(months == m)[1]) # it then takes the LOCATION of the FIRST VALUE equal to m
}
ids <- gd_properties(x)$id[ids] # this gets the image ids at those locations
ids # Now we have all the ids we want to download!
```




## Here's how I would do this


```{r}
#| eval: false
#| echo: true
#| code-fold: show
#| class-output: hscroll

adminareas <- malawi |>
  as_tibble() |>
  select(EA_CODE)

for (i in 1:length(ids)){
  x <- gd_image_from_id(ids[i]) |>
  gd_download(
    filename = "temp.tif",
    region = bbox, # region is our bbox
    scale = 500, # resolution
    crs = 'EPSG:4326', # lat/lon
    overwrite = TRUE, # overwrite if it exists
    bands = list("NDVI") # only download NDVI
  )
  x <- rast(x) # load raster
  ndviextract <- exact_extract(x, malawi, fun = "mean", append_cols = "EA_CODE")
  colnames(ndviextract) <- c("EA_CODE", paste0("NDVI_", i))
  adminareas <- adminareas |>
    left_join(ndviextract, by = "EA_CODE")
}

```





## But that's difficult, so I've uploaded the data!

- That's a hard thing to wrap your head around if you're new to R
  - If you want to download them, you can just do them one at a
  - I've uploaded the data:
    - `day4data/ndviallmonths.csv`
    - Go ahead and load it and take a look at it

```{r}
#| eval: true
#| echo: true
#| code-fold: true
#| class-output: hscroll

ndviall <- read_csv("day4data/ndviallmonths.csv")
ndviall

```





## Let's create some new variables

- Let's create three new NDVI variables:
  - Annual minimum
  - Annual maximum
  - Annual average
- New `R` function: apply!

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

# create new variable called min. the "1" means across ROWS.
ndviall$ndvimin <- apply(ndviall |> select(starts_with("NDVI")), 1, min, na.rm = TRUE)
# max
ndviall$ndvimax <- apply(ndviall |> select(starts_with("NDVI")), 1, max, na.rm = TRUE)
# mean
ndviall$ndvimean <- apply(ndviall |> select(starts_with("NDVI")), 1, mean, na.rm = TRUE)
# just keep those
ndviall <- ndviall |>
  select(EA_CODE, ndvimin, ndvimax, ndvimean)
# save it!
write_csv(ndviall, "day4data/ndviclean.csv")

```







## One last step

- We need to get the codes for the survey data!
- I've uploaded the survey data for Malawi
  - `day4data/ihs5_consumption_aggregate.dta`
  - Go ahead and load it (remember to use `haven`)

```{r}
#| eval: true
#| echo: true
#| code-fold: true
library(haven)
ihs5 <- read_dta("day4data/ihs5_consumption_aggregate.dta")
head(ihs5)
```









## One last step

```{r}
#| eval: true
#| echo: true
#| code-fold: show
head(ihs5)
```

- Note that in this case, we already have the ea codes in the survey data
  - So we can just join the two datasets!









## Collapse to EA_CODE

```{r}
#| eval: true
#| echo: true
#| code-fold: show
library(stats) # this is for weighted.mean
ihs5ea <- ihs5 |>
  rename(EA_CODE = ea_id) |>
  group_by(EA_CODE) |>
  # Note that this is a weighted mean!
  summarize(poor = stats::weighted.mean(x = poor, w = hh_wgt*adulteq, na.rm = TRUE), # weighted mean
    total_weights = sum(hh_wgt*adulteq, na.rm = TRUE), # sum total weights
    total_obs = n()) # total observations (households) in the EA
head(ihs5ea)
```









## But what if you don't have an identifier?

- Sometimes you have GPS coordinates but not matching identifiers
  - Good news! We can use geospatial tools to match the coordinates to the shapefile
  
```{r}
#| eval: true
#| echo: true
#| code-fold: show
ihs5coords <- read_dta("day4data/householdgeovariables_ihs5.dta")
# turn it into an sf object
ihs5coords <- ihs5coords |>
  filter(!is.na(ea_lon_mod)) |> # get rid of any missing values (can't use them)
  st_as_sf(coords = c("ea_lon_mod", "ea_lat_mod"), crs = 4326)
head(ihs5coords)
```









## Here's how it looks


::: columns
::: {.column width="65%"}

```{r}
#| eval: false
#| echo: true
#| code-fold: show
ihs5coords <- read_dta("day4data/householdgeovariables_ihs5.dta")
# turn it into an sf object
ihs5coords <- ihs5coords |>
  filter(!is.na(ea_lon_mod)) |> # get rid of any missing values (can't use them)
  st_as_sf(coords = c("ea_lon_mod", "ea_lat_mod"), crs = 4326)
admin4 <- read_sf("day4data/mw4.shp")
ggplot() + 
  geom_sf(data = admin4, color = "transparent", lwd = 0.01) +
  geom_sf(data = ihs5coords, color = "red") +
  theme_minimal()
```

:::

::: {.column width="35%"}

![](day4assets/ihs5coords.png){fig-align="center" width="65%}

:::
:::










## We can join them using `st_join`

```{r}
#| eval: true
#| echo: true
#| code-fold: show
ihs5coords <- read_dta("day4data/householdgeovariables_ihs5.dta")
# turn it into an sf object
ihs5coords <- ihs5coords |>
  filter(!is.na(ea_lon_mod)) |> # get rid of any missing values (can't use them)
  st_as_sf(coords = c("ea_lon_mod", "ea_lat_mod"), crs = 4326)
admin4 <- read_sf("day4data/mw4.shp")
ihs5coords <- st_join(ihs5coords, admin4)
# and that's it!
head(ihs5coords)
```









## Now we need to add the EA_CODE to the poverty data

```{r}
#| eval: true
#| echo: true
#| code-fold: show
ihs5 <- read_dta("day4data/ihs5_consumption_aggregate.dta")
ihs5 <- ihs5 |>
  left_join(ihs5coords, by = "case_id")
# collapse to EA_CODE
ihs5ea <- ihs5 |>
  group_by(EA_CODE) |>
  # Note that this is a weighted mean!
  summarize(poor = stats::weighted.mean(x = poor, w = hh_wgt*adulteq, na.rm = TRUE), # weighted mean
    total_weights = sum(hh_wgt*adulteq, na.rm = TRUE), # sum total weights
    total_obs = n()) # total observations (households) in the EA
head(ihs5ea)
# save it!
write_csv(ihs5ea, "day4data/ihs5ea.csv")

```











# One last set of data: MOSAIKS

## {background-image="day4assets/mosaikbackground.png"}





## One last set of data: MOSAIKS

- MOSAIKS is a dataset that has a lot of different variables
  - These variables were constructed by the authors from satellite imagery
  - You can download all of these features using their website
  - [https://api.mosaiks.org](https://api.mosaiks.org) - you'll have to register
  - The data is quite large, so I've downloaded and uploaded it for you
    - It's a random selection of 500 variables for EAs in Norther Malawi
    - `day4data/mosaikvars.csv`
    - Note that I used `st_join` to match it to the shapefile!

[Rolf et al. (2021)](https://www.nature.com/articles/s41467-021-24638-z)








# Putting it all together



## What have we downloaded?

- We have:
  - Population data from WorldPop
  - Nightlights data from GEE
  - NDVI data from GEE
  - MOSAIKS data
  - Survey data from Malawi<br><br>
  
- We have everything we need to estimate a simple SAE model using geospatial data!
  - Note: in practice, I would use even more predictors, but this is a good start



## First, let's load all the data

```{r}
#| eval: true
#| echo: true
#| code-fold: show

# Population 
pop <- read_csv("day4data/mwpopEAs.csv")
# Nightlights
ntl <- read_csv("day4data/mwntlEAs.csv")
# NDVI
ndvi <- read_csv("day4data/ndviclean.csv")
# mosaiks
mosaik <- read_csv("day4data/mosaikvars.csv")
# survey data
ihs5ea <- read_csv("day4data/ihs5ea.csv")
# all EAs
adm4 <- read_sf("day4data/mw4.shp")
# no geometry, just EA_CODE
adm4 <- as_tibble(adm4) |>
  dplyr::select(EA_CODE, TA_CODE)

```





## Now, we join it all!

```{r}
#| eval: false
#| echo: true
#| code-fold: show

adm4 <- adm4 |>
  left_join(ihs5ea, by = "EA_CODE") |> # add survey data
  left_join(pop, by = "EA_CODE") |> # add pop
  left_join(ntl, by = "EA_CODE") |> # add nightlights
  left_join(ndvi, by = "EA_CODE") |> # add ndvi
  left_join(mosaik, by = "EA_CODE") |> # add mosaik
  
head(adm4)

```
- Oh no! This throws an error!
  - `x$EA_CODE` is a `<character>`
  - `y$EA_CODE` is a `<double>`
  - What's going on?





## Now, we join it all!

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

adm4 <- adm4 |>
  mutate(EA_CODE = as.numeric(EA_CODE)) |>
  left_join(ihs5ea |> mutate(EA_CODE = as.numeric(EA_CODE)), by = "EA_CODE") |> # add survey data
  left_join(pop |> mutate(EA_CODE = as.numeric(EA_CODE)), by = "EA_CODE") |> # add pop
  left_join(ntl |> mutate(EA_CODE = as.numeric(EA_CODE)), by = "EA_CODE") |> # add nightlights
  left_join(ndvi |> mutate(EA_CODE = as.numeric(EA_CODE)), by = "EA_CODE") |> # add ndvi
  left_join(mosaik |> mutate(EA_CODE = as.numeric(EA_CODE)), by = "EA_CODE") # add mosaik
  
head(adm4)

```








## What do poverty rates look like?

```{r}
#| eval: true
#| echo: true
#| code-fold: show

# levels vs. arcsin squareroot transformation
ggplot() +
  geom_density(data = adm4, aes(x = poor), fill = "navy", alpha = 0.5) +
  geom_density(data = adm4, aes(x = asin(sqrt(poor))), fill = "orange", alpha = 0.5) +
  theme_minimal()
# Not perfect but neither is horribly non-normal!

```










## Before SAE, need to do some cleaning

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

# check for missings
sum(is.na(adm4$pop))
sum(is.na(adm4$ntl))
sum(is.na(adm4$ndvimin))
sum(is.na(adm4$mosaik1))
# we have missings for the mosaik features!


```









## Missing mosaiks, what to do?

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

# I am going to replace missing mosaiks values with the TA mean
adm4 <- adm4 |>
  group_by(TA_CODE) |>
  mutate(across(starts_with("mosaik"), ~replace_na(., mean(., na.rm = TRUE)))) |>
  ungroup()

sum(is.na(adm4$mosaik1)) # fixed!
```









## Now let's select features

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll
library(glmnet)

# let's also log population!
adm4 <- adm4 |>
  mutate(pop = log(pop))

surveyeas <- adm4 |>
  filter(!is.na(poor))
samplefeatures <- surveyeas |>
  select(-c(EA_CODE, TA_CODE, poor, total_weights, total_obs)) # remove these variables
samplelabels <- surveyeas$poor
sampleweights <- surveyeas$total_weights

# now lasso
set.seed(24826)
lasso <- cv.glmnet(x = as.matrix(samplefeatures), y = samplelabels, 
  weights = sampleweights,
  alpha = 1)

```









## What did lasso do?

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

coef(lasso, s = "lambda.min")

```









## We want to get the non-zero variables

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

coefs <- coef(lasso, s = "lambda.min")
nonzerocoefs <- row.names(coefs)[which(coefs!=0)]
nonzerocoefs
# but we don't want the intercept here (you'll see why)
nonzerocoefs <- nonzerocoefs[-1]
nonzerocoefs
```








## Now we want to turn that into a formula!

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

formula <- as.formula(paste("poor ~", paste(nonzerocoefs, collapse = " + ")))
formula
# you can see it works with a simple linear regression!
lm(formula, data = surveyeas)
```









## Finally, we can do SAE

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll
library(povmap)

saeresults <- povmap::ebp(fixed = formula, # the formula
  pop_data = adm4,
  pop_domains = "TA_CODE",
  smp_data = surveyeas,
  smp_domains = "TA_CODE",
  transformation = "arcsin",
  weights = "total_weights",
  pop_weights = "pop",
  weights_type = "nlme",
  MSE = TRUE,
  rescale_weights = TRUE,
  seed = 1234,
  L = 0)

```








## The results

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

# What is R2?
summary(saeresults)$coeff_determ

```

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

# Relatively normal?
summary(saeresults)$normality

```







## Now let's get the predictions

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

# get the predictions
hat <- as_tibble(saeresults$ind)
hat <- hat |>
  select(TA_CODE = Domain, poor = Mean)
# We also want variance estimates
mse <- as_tibble(saeresults$MSE) |>
  select(TA_CODE = Domain, mse = Mean)
# note to get the standard error we have to take the square root!
mse <- mse |>
  mutate(se = sqrt(mse))

# finally, join them!
final <- left_join(hat, mse, by = "TA_CODE")

head(final)
```







## Let's map it!

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

# Admin3 (TA)
adm3 <- read_sf("day4data/mw3.shp")
adm3 <- adm3 |>
  left_join(final, by = "TA_CODE")

# let's output two things: poverty rate and CV (se/mean)
g1 <- ggplot() +
  geom_sf(data = adm3, aes(fill = poor), color = "white") +
  scale_fill_distiller("Poverty Rate", palette = "Spectral", direction = -1) +
  theme_minimal() +
  theme(legend.position = "bottom")
g2 <- ggplot() +
  geom_sf(data = adm3, aes(fill = se/poor), color = "white") +
  scale_fill_distiller("CV", palette = "Spectral", direction = -1) +
  theme_minimal() +
  theme(legend.position = "bottom")
g3 <- ggplot() +
  geom_sf(data = adm3, aes(fill = as.factor((se/poor)<=0.4)), color = "white") +
  scale_fill_brewer("CV below 0.4?", palette = "Reds", direction = -1) +
  theme_minimal() +
  theme(legend.position = "bottom")

```






## The final result!

```{r}
#| eval: true
#| echo: true
#| code-fold: true
#| fig-width: 17
library(cowplot)

plot_grid(g1, g2, g3, ncol = 3)
```






## And here it is with Rumphi

```{r}
#| eval: true
#| echo: true
#| code-fold: true
#| fig-width: 17

rumphibbox <- st_bbox(adm3 |> filter(DIST_CODE=="107"))
g2new <- ggdraw() +
  draw_plot(
    {
      g1 +
        coord_sf(
          xlim = c(rumphibbox[1], rumphibbox[3]),
          ylim = c(rumphibbox[2], rumphibbox[4]),
          expand = FALSE) +
        labs(subtitle = "Rumphi only")
    }
  )
g1new <- g1 + 
  labs(subtitle = "Northern Malawi") +
  geom_sf(data = st_as_sf(st_as_sfc(rumphibbox)), fill = NA, color = "black")
plot_grid(g1new, g2new, ncol = 2)

```



















